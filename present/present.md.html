<meta charset="utf-8" emacsmode="-*- markdown -*-">

# Introduction

- Danke für die Vorstellung, ich bin ... 
- in meiner MAsterarbeit ging es darum, Gemeinsamkeiten in 2 Analysen im GHC herauszuarbeiten
- ... warum

# Motivation

- 1 Minute
- GHC altes und mittelgroßes Softwareprojekt, >100k Zeilen Code, nicht immer einfach
- Bringt die typischen softwaretechnischen Herausforderungen mit sich
- Vor dem Hintergrund ... angesprochenen zwei Analysen, Call Arity und Demand Analysis, ... in Logik überlappen
- Allein dem Prinzip DRY folgend macht es Sinn die gemeinsame Logik zu extrahieren
- u.U. können beide Analysen von den präziseren Ergebnissen der kombinierten Analyse profitieren

# Cardinality Analysis

- 2 Minuten
- Bevor wir uns den beiden Analysen zuwenden, beschäftigen wir uns zunächst mit Kardinalitätsanalysen
- KA berechnet untere und obere Schranken dafür, wie oft etwas ausgewertet wird
- In Anlehnung an Alias Analyse  
  - May und MustAlias unterscheiden
  - in MinCard (untere SChranken) und MaxCard (nur obere Schranken) aufteilen
- Zur Notation: Intervallschreibweise, interessante Kardinalitäten nur 0,1,mehrmals
- Beispiel, was eine Kardinalitätsanalyse herausfinden kann:  
  - a wird mindestens einmal, möglicherweise mehrmals ausgewertet
  - b wird genau einmal ausgewertet
  - c möglicherweise gar nicht oder mehrmals
  - d möglicherweise einmal, aber nicht mehrmals
  - e ist tot
- Aus Zeitgründen muss ich leider sämtliche Optimierungen überspringen

# Usage Analysis

- 1 Minute
- Damit sind wir schon bei Usage Analysen, dem eigentlichen Thema meiner Arbeit
- Usage Analysen berechnen MaxCard Informationen, also obere Grenzen zu Auswertungskardinalitäten von Bindings
- Verwandt: Identifikation von one-shot lambdas, also lambdas, die höchstens einmal aufgerufen werden  
  - genauer: Anzahl an Auswertungen des Körpers relativ zu einer einzigen Auswertung des Lambda Ausdrucks
- f wird zweimal aufgerufen, daher äußeres Lambda mit $\omega$ annotiert
- Jede partielle Funktionsanwendung wird aber nur einmal aufgerufen, daher inneres lambda in beiden Fällen one-shot  
  - falls nicht verstanden: Partielle Funktionsanwendung wertet einmal zu innerem lambda Ausdruck aus, der dann relative genau einmal aufgerufen wird
- g dagegen wird nur einmal aufgerufen, daher alle lambdas one-shot
- Ermöglicht wichtige Optimierungen:  
  - let floating
  - eta expansion
  - inlining

# Demand Analysis

- 1 Minute
- Damit haben wir alle Grundlagen zusammen, um die beiden Analysen zu besprechen
- GHC's Demand Analyser ist eine Kardinalitätsanalyse (berechnet also obere und untere Grenzen für Auswertungskardinalitäten)
- Insbesondere bemüht der Demand Analyser eine Usage Analyse, um single-entry thunks, one-shot lambdas und tote Ausdrücke zu identifizieren
- Berechnet für jeden Ausdruck, wie er seine freien Variablen benutzt
- Rückwärtsanalyse
- Drei Besonderheiten zu besprechen

# Call Uses

> 1 Minute

- Bis jetzt haben unsere Analysen immmer nur bestimmt, wie oft ein Binding ausgewertet wurde
- Die Information, *wie* ein Binding ausgewertet wurde ist aber auch wichtig, um one-shot lambdas zu identifizieren
- Call uses leisten genau das
- Im Beispiel:  
- Hier wird f folgender Usage ausgesetzt: höchstens einmal ausgewertet, dann höchstens einmal aufgerufen und das Ergebnis höchstens einmal aufgerufen
- Im zweiten Beispiel wird f mehrmals ausgewertet, mehrmals aufgerufen, aber die partiellen Applikationen in allen Fällen höchstens einmal aufgerufen
- Die one-shot annotationen lassen sich dann genau an den Call Uses ablesen

# Usage Signatures

2 Minuten

- Demand Analyser ermöglicht interprozeduralen Datenfluss über Signaturen, die angeben wie eine Funktion ihre Argumente benutzt
- Jeder Ausdruck wird approximiert durch Usage auf freie Variablen + UsageSig = Usage Type
- Um UsageSignaturen an den Call Sites verfügbar zu haben, wird der gebundene Ausdruck vor dem let body analysiert, top-down
- Für Funktionen wird also die LetDn Regel benutzt: Usage types werden an Benutzungsstellen frei gelassen
- Hier: Const benutzt nur sein erstes ARgument, daher die Usage Signature
- An der Aufrufstelle wird dann y als einmal benutzt identifiziert und fac 1000 als tot

# Thunks

1 Minute

- Bei Thunks funktioniert LetDn nicht so gut, weil die Auswertung der rechten Seite zwischen Aufrufen geteilt wird
- hier hat `y` zwei Benutzungsstellen, mit LetDn daher zwei Benutzungen von `x`
- Die Auswertung der rechten Seite von `y` wird aber geteilt! Nur eine Benutzung von `x`
- Daher bei Thunks LetUp: Erst let body analysieren, Usage merken und dann rechte Seite in dem festgehaltenen Use, hier $U$, analysieren
- Das wäre schon alles wichtige zum Demand Analyser (vielleicht Fragen?)

# Call Arity

3 Minuten

1. Grundsätzliche Idee: Eta-Expansion basierend darauf, wie Bindings benutzt werden  
  - Hier: `f` wird immer mit zwei Argumenten aufgerufen, Sharing ist so nie sichtbar
2. `f` kann zu Arity 2 expandiert werden
3. Wie sieht das in dieser Situation aus?
  - `f` wird zwar immer noch mit 2 Argumenten aufgerufen
4. Aber `f` ist auch ein Thunk, daher wird zwischen den Aufrufen die Arbeit des `if` Ausdrucks geteilt  
  - Insbesondere die Auswertung von `expensive`, die beliebig teuer sein kann
  - Eta-Expansion würde Sharing zunichte machen und den teuren Ausdruck mehrmals auswerten
5. Hier ist `f` wieder ein Thunk, mit zwei Argumenten aufgerufen  
6. Und weil `f` nur einmal aufgerufen wird, wird kein Sharing sichtbar  
  - Eta-Expansion ist sicher
  - Insbesondere zeigt das Beispiel auch, dass selbst in Situationen in denen statisch klar ist, dass `f` nur einmal aufgerufen wird, Inlining trotzdem nicht immer möglich ist
  - Called Once information also wichtig (für unsere Zwecke wie single-entry)

# Call Arity (Co-call graphs)

2-3 Minuten

1. Called once information wird durch eine Usage Analyse berechnet  
  - Damit rechte Seite in abhängigkeit von Anzahl Argumenten analysiert werden kann, muss body vor RHS analysiert werden
  - Wie LetUp bottom-up
  - LetUp aber zu unpräzise, wie dieser Fall zeigt
  - Hier wird vergessen, dass x und y nie zusammen ausgewertet werden
  - dann wird die rechte Seite von y freigelassen: x wird mehrfach benutzt
2. Deswegen hat sich Joachim Co-call Graphen ausgedacht
  - Merken sich Kontrollflussinformationen
  - Kanten verbinden variablen, die möglicherweise zusammen ausgewertet werden
  - Andersherum: Nichtvorhandensein einer Kante schließt gemeinsame Auswertung aus
  - Schleifen modellieren called-once information
  - Hier: Für den if Ausdruck haben wir Co-Call Graph gegeben
  - x und y zusammen mit b ausgewertet, aber keine Kante zwischen beiden
  - An let bindings findet subsitutionsschritt statt
3. Usage type von y (besteht hier nur aus einem einzelnen Knoten x) wird hier in die Vorkommnisse von y im Typen vom Body eingesetzt und alle Knoten im Graphen mit allen Nachbarn von y verbunden
4. Da x nicht mit y verbunden war, folgt daraus keine Schleife an x

# Combined Usage Analysis

1 Minute

- Idee für kombinierte Usage Analyse:  
  - Nimm Usage Analyse vom Demand Analyser
  - Füge Co-Call graphen hinzu, um single-entry thunks zu verfolgen
  - Im Fall von Funktionen (also LetDn) werden präzisere call uses an die rechten Seiten weitergegeben
- Was das heißt, sehen wir am besten an einem Beispiel

# Call Arity vs. Demand Analysis

4 Minuten

1. Schauen wir uns mal das untere Beispiel an  
  - Wichtig ist hier, wie oft `z` ausgewertet wird, tatsächlich nur einmal
  - Demand Analysis analysiert das let binding aber mit der LetDn Regel, also top-down, weil `f` kein Thunk ist
  - Um einen Usage type für `f` parat zu haben, nimmt Demand Analysis einen aufruf mit manifest arity an, das ist die Anzahl führender Lambdas, hier 1
  - Das reicht aber nicht, um 'unter das innere Lambda' zu gucken
  - Muss daher konservativ annehmen, dass `z` mehrfach ausgewertet wird
  - Tatsächlich wird `f` aber mit zwei Argumenten aufgerufen, dieser Use hier
  - Hätte man die rechte Seite vorher in dem Use analysiert, würde man dsa gewünschte Ergebnis erzielen
2. Call Arity erbringt hier das gewünschte, weil die nötige Arity information von unten nach oben fließt (LetUp, hat dafür keine Usage Signaturen)  
  - Meine Lösung: Rechte Seite erst analysieren wenn gefordert, dann ist der präzise Use bekannt
  - Erste Ideen basierten darauf, irgendwie laziness auszunutzen
  - Das habe ich aber schnell wieder verworfen
3. Denn sobald Rekursion ins Spiel kommt, bekommt man Terminationsprobleme  
  - Hier: `fac` ruft sich rekursiv auf. Um `fac`s usage type zu bestimmen, müsste man `fac`s usage type kennen
  - Wird sehr schnell sehr kompliziert, sogar noch ohne wechselseitige Rekursion
  - So habe ich Fixpunktitertaion wiedergefunden, deren Iterationsreihenfolge aber nicht wie üblich an den Syntaxbaum gekoppelt ist, sondern gerne auch mal hin- und herspringt
  - Das hat dann zu einer Graphabstraktion geführt, bei der Transferfunktionen syntaktischer Elemente Knoten zugeordnet werden
  - Lösung wird durch einen Worklistalgorithmus bestimmt
  - Wie wir gesehen haben, ist der Usage Type eines Ausdrucks immer auch abhängig von dem Use (Anzahl Argumente des Aufrufs) unter dem die Expression steht

# Transferfunktion

1.5 Minuten

- Formal denotieren wir Ausdrücke durch monotone usage transformer
- Ein Usage Transformer bildet den eigehenden use auf den zugehörigen usage type (also wie freie Variablen und Argumente benutzt werden) ab
- Wie wir eben gesehen haben, ist Anzahl eingehender Argumente wichtige Information
- Nur um einmal gezeigt zu haben, wie das dann formal aussieht, hier die Transferfunktion für nicht-rekursive let bindigns
- TRansferfunktion ordnet jedem Ausdruck mithilfe einer Umgebung von usgae transformern für Aufrufe von freien Variablen einen usage transformer zu
- Ohne tiefer ins detail zu gehen, hier wird der usage transformer der rechten Seite der Transformer Umgebung als LetDn-Variante zugefügt
- Hier wird dann die rechte Seite analysiert basierend auf dem use den der body ausübt (LetUp)

# Recovering Call Arity

2.5 Minuten

1. Meine Analyse verallgemeinert die Usage Analyse im Demand Analyser und ersetzt Call Arity
- Daher muss man die Call Arity Information auch aus den USage resultaten gewinnen können
- Wichtige Beobachtung: Eta-Expansion ändert nichts daran, wie Ausdrücke benutzt werden
- Hier sieht man den Algorithmus den ich benutze, um bindings basierend auf usag einformationen zu eta-expanden
- Leichter an ein den Beispielen zu Call Arity zu verstehen
2. `f` ist hier eine Funktion unter multi-call usage  
  - `f` ist kein Thunk, also ist die Auswertungsmultiplizität uninteressant
  - ab da kann man entsprechend der manifest arity (also der anzahl führender lambdas) viele Cs überspringen, hier 1
  - wir dürfen nun so lange expandieren, wie wir single-call uses appellen können, hier 1
3. also eta-expansion zu arity 2
4. Ähnliches Beispiel, diesmal ein Thunk in multi-call usage  
  - Hier scheitert es schon an der Auswertungsmultiplizität $\omega$ (die für unsere Fälle tatsächlich immer kleiner oder gleich des äußeren call use ist)
  - Eta-expansion würde Arbeit duplizieren!
5. Ein Thunk in single-call usage dagegen...  
  - Hier ist die Auswertungsmultiplizität 1, wir dürfen weiter machen
  - Arity ist 0, wir dürfen also keine führenden Call uses entfernen
6. Äußeres lambda one-shot! wir dürfen expandieren
7. Inneres lambda auch one-shot! nochmal expandieren
8. Vollständig eta-expandiert zu arity 2, wie Call Arity

# Implementation

1 Minute

- Noch ein paar Worte zur Implementierung
- Gerade der Demand Analyser ist ein ziemlich zentraler Bestandteil vom GHC, Bugs haben sich an den unmöglichsten Stellen bemerkbar gemacht
- Ansonsten führte eins zum anderen
- Co-Call graphs müssen jetzt mit wesentlich mehr variablen umgehen, daher neue Repräsentation die effizient volle und dünne Graphen darstellt
- Graphbasierte Fixpunktiteration hat mich einiges an Nerven gekostet
- Monotonie der approximierten usage transformer wichtig für Termination! Es existiert keine geeignete datenstruktur
- (Map, die nicht totale Ordnung der Schlüssel sondern nur Verband vorraussetzt)

# Evaluation

4 Minuten für alle overlays

- Ich stelle hier zwei der gebenchmarkten Varianten vor, relativ zur baseline GHC 8.2.1
- Variante 1: Volle Analyse, mit co-call graphen und den Veränderungen an LetDn
- Variante 2: Keine co-call graphen, trotzdem LetDn
- Als Metrik dienen Allokationen und die Anzahl ausgeführter Instruktionen, mit cachegrind gemessen
- Kompilat Performance anhand der nofib Benchmark suite, Standard im GHC
- Compiler Perofmrnace während die Benchmark suite kompiliert wurde, sowie während der stage 2 compiler mit meinem optimierten stage 1 Compiler kompiliert wurde
- GHC bootstrapping: bootstrpa compiler kompiliert stage 1 Compiler, der stage 1 Compiler kompiliert alle nötigen libraries und den stage 2 Compiler, der dann verwendet wird

# Allocations

- Erfreulicherweise keine Regressionen, Ziel erreicht
- aber auch keine großen Verbesserungen
- Co-Call Graphen haben interessanterweise keinen großen Einfluss, wenn die verbesserte LetDn REgel aktiv ist
- Tatsächlich wird auch das List fusion Beispiel, dass Call Arity motiviert, ohne co-call graphen optimiert

# Compiler Performance

- Wie die ZAhlen zeigen, hat sich die Performance des Compilers durchgehend verschlechtert
- Ist auch nicht verwunderlich, denn wir ersetzen einen Lauf von Call Arity durch 3 Läufe einer Analyse, die Call Arity verallgemeinert
- Außerdem sei noch gesagt, dass der Speicherverbrauch von der Variante mit co-call graphen für eine Eingabedatei so in die Höhe geschossen ist, dass die 32GB der Referenzmaschine nicht zum kompilieren ausreichten
- daher wurde der stage 2 Compiler ohne optimierungen kompiliert, was sich auch in den Werten widerspiegelt
- Auch wenn es die Zahlen nicht hergeben, gefühlt ist die Performance von Variante (2) deutlich besser und vor allem konsistenter

# Related Work (Backup?)

- 2 Minuten

- Ansätze kann aufteilen in welche die über Abstrakte Interpretation funktionieren und solche, die das problem über typsysteme angehen
- Beide hier beschriebenen Analysen funktionieren über abstrakte Interpretation
- In den 80ern und 90ern hat viel Forschung zu Strictness Analyse stattgefunden (MinCard)
- Das hat letztendlic auf den Ansatz von projection transformers geführt, die später demand transformer und damit meine usage transformer inspiriert haben
- Seit den 90ern bis heute wird an Typsystemen zur Usage Analyse geforscht
- Theoretisch größere Präzision, gerade im interprozeduralen Fall
- Aber praktisch erweist sich die typische Kombination von Polymorphismus und Subtyping (subeffecting) verbunden mit der Explosion von zu unifizierenden Variablen, die durch Annotation von Datentypen hervorgerufen wird als zu langsam heraus

# Conclusion

1.5 Minuten

- Ich habe eine Usage Analyse geschrieben, die ihr Gegenstück im Demand Analyser und Call Arity verallgemeinert  
  - Als 'Beweis' für die Korrektheit führe ich die fehlenden Regressionen in der Benchmarksuite an
  - hat sich aber auch nicht viel verbessert
- Ich müsste teilweise ziemlich tief im GHC wühlen, bis ich alle nötigen Stellen gefunden habe, die ich ändern musste
- Für die Zukunft erweisen sich die Graph-basierte Fixpunktiteration als interessantes Forschungsthema
- Auch fehlt eine geeignete Datenstruktur für (montone) Maps mit join-semilattices als schlüssel
- Auf lange Sicht wäre eine Integration in GHC denkbar, im Moment allerdings zu viel Neues
- GHC könnte aber von den Ideen aus meiner Arbeit profitieren

<hr/>

# Zeit

- Grundlagen (Motivation (1) + KA (2) + Usage Analysis (1) + Puffer (1)) = 5
- Demand Analysis (1 + CU (1) + UsageSig (2) + Thunks (1)) = 5
- Call Arity (2 + 3) = 5
- Combined Usage Analysis (1) , Call Arity vs. Demand Analysis (4) = 5
- Transferfunktion (1.5), REcovering Call Arity (2.5), Implementierung (1) = 5
- Evaluation (3), Related Work (2), Conclusion (1.5) = 6.5

- 31.5 Minuten

<hr/>

# Backup

## Call-by-value (wahrscheinlich Backup)

- 1 Minute
- Wir können den Overhead vermeiden, der durch lazy evaluation (call-by-need) entsteht
- Strikte bindings direkt ausgwertet werden
- erlaubt weitere wichtige Optimierungen wie Unboxing von primitiven Typen um Allokationen komplett zu vermeiden
- Blind die Auswertungsstrategie in Call-by-value ändern beeinflusst Terminationsverhalten
- Daher: 'Welche bindings werden mind. einmal ausgewertet?'
  - typische MinCard Frage, die von Strictness Analyse beantwortet wird
- Hier: a und b sind strikt, was durch das Bang Pattern angedeutet wird

## Call-by-name (Backup?)

- 1 Minuten
- In bestimmten Fällen kann der Compiler auch call-by-name statt call-by-need verwenden
- Compiler verpflichtet sich, rechte Seiten von Bindings nur einmal auf WHNF auszuwerten
- Dazu muss Call-by-need einmal berechnete Ergebnisse von sogenannten Thunks memoisieren
- Wenn so ein Thunk nur höchstens einmal ausgewertet wird, ist das unnötig
- solche single-entry thunks kann man in Funktionen verwandeln, um Thunk updates zu sparen
- Dazu: 'Welche Bindings werden höchstens einmal ausgewertet'  
  - MaxCard Frage, beantwortet durch Sharing Analysen
- Im Beispiel durch zusätzliches unit argument angedeutet: b, d, e

## Eta-expansion (Backup?)

- < 1 Minute
- Optimierung, die versucht, lambdas nach außen zu heben
- Ist nicht immer sicher, weil Auswertungsarbeit von d hier dupliziert werden könnte
- Weil zu hebendes Lambda aber one-shot ist, dürfen wir das! 
- Der Körper wird nicht öfter ausgeführt als der umgebende Ausdruck

## Executed Instructions (Backup?)

- Schaut man auf die Anzahl ausgeführter Instruktionen sind die Unterschiede größer
- Viele Ausreißer (keine Fluktuationen), obwohl ich keine Unterschiede im generierten Code feststellen konnte
- Insgesamt gereicht es aber zu unserem Vorteil (-0.6%)
- Co-Call graphen scheinen in isolierten Fällen einen kleinen Unterschied zu machen


<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
