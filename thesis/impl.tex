\chapter{Implementation}\label{sec:impl}

This chapter is concerned with the implementation of a usage analysis as specified in \cref{sec:spec} within the Glasgow Haskell Compiler (GHC).

A detailed walkthrough of the Haskell code is out of scope and would not be particularly interesting, so we will instead discuss design decisions and interesting problems we encountered.

\section{Object Language}

After a Haskell program passes through GHC's frontend, it is compiled down to an explicitly typed core calculus called GHC Core. 
Core is the first of a number of intermediate languages the program is translated to before an executable artifact is produced.
Already being vastly simpler than Haskell's huge surface syntax, as can be seen in \cref{fig:core}, Core is still quite complex compared to the object language introduced in \cref{sec:exp}.

While Core is still unconcerned with operational details, most optimisations within GHC are realised as Core-to-Core passes.
A non-strict, high-level language like Haskell provides ample opportunities for optimisation even in this macroscopic context.
The representation as a lambda calculus allows simplification based on term rewriting, which GHC makes great use of in its simplifier.
Functional languages encourage composition of concise definitions, so GHC supports its optimisations with an aggressive inliner.

Apart from the simplifier, GHC employs other transformations which rely on precise information made available by analyses like GHC's Demand Analyser and Call Arity.
While the Demand Analyser combines strictness analysis \parencite{dmd} with a usage analysis \parencite{card} and constructed product result analysis \parencite{cpr}, Call Arity is an arity analysis interleaved with a sharing analysis based on co-call graphs, to find out which bindings can be $\eta$-expanded without losing any shared work \tod{move this elsewhere?}.

As is the case for Demand Analysis and Call Arity, our usage analysis will operate on and annotate GHC Core expressions.

\begin{figure}[h]
  \begin{haskellcode}
    data Expr b
      = Var      Id
      | Lit      Literal
      | App      (Expr b) (Expr b)
      | Lam      b (Expr b)
      | Let      (Bind b) (Expr b)
      | Case     (Expr b) b Type [Alt b]
      | Cast     (Expr b) Coercion
      | Tick     (Tickish Id) (Expr b)
      | Type     Type
      | Coercion Coercion

    type Alt b = (AltCon, [b], Expr b)

    data AltCon
      = DataAlt DataCon
      | LitAlt  Literal
      | DEFAULT 

    data Bind b 
      = NonRec b (Expr b)
      | Rec [(b, (Expr b))]
  \end{haskellcode}
  \caption{Part of the data types representing the syntax of GHC Core}
  \label{fig:core}
\end{figure}

\section{Top-level Bindings}\label{sec:toplvl}

The code of a module in GHC Core is represented as a list of top-level definitions, some of which are exported.

To avoid duplication with the treatment of \keyword{let} bindings, we translate the list of definitions into an expression of nested \keyword{let}s before the analysis and back after the analysis.

Which usage are top-level bindings exposed to? 
For exported bindings, we don't oversee all potential use sites, so we have to be conservative and assume $\omega*U$. 
Exported bindings are similar to garbage collection roots: 
All non-absent bindings must be reachable through an exported binding. 
This is because for non-exported top-level bindings, their whole scope is known.

Based on this observation, it is also clear what the expression within the innermost \keyword{let} should be: A tuple of the exported identifiers.
This encoding of modules is common-place in languages like JavaScript (by the name of \emph{Revealing module pattern}) that lack(-ed) a proper module system.

Considering exported identifiers as roots is necessary, but, as it turned out, not sufficient.
GHC's rewrite rules and vectorisation declarations possibly mention identifiers which are neither exported, nor reachable otherwise, so these must be included in the root set.

A similar problem occurs for \emph{unfoldings}. 
Unfoldings enable inlining across module boundaries by serialising the \emph{unoptimised} bound expression into the module's interface file, a Haskell-specific compilation artifact like object files.
These unfoldings play a crucial role in revealing opportunities for custom rewrite rules.

Because unfoldings consist of the unoptimized bound expressions, they potentially reference bindings which are already optimised away or replaced by an optimised variant in the actual object code. 
As for rewrite rules and vectorisation declarations, ignoring unfoldings can result in surprising behavior and unforeseen crashes due to execution of supposedly absent code.

Correct handling of unfoldings would require to treat them as alternative right-hand sides of the binding they decorate.
Experimental support for unfoldings in the style of \hsinl{if True then rhs else unfolding} resulted in scoping issues of inner bindings, as well as distortions of analysis results.
As we didn't observe any crashes related to unfoldings when compiling and running the entire compiler, test suite and benchmark suite, we postponed proper handling of the problem.

Of course, the simplest sufficient root set would be to include all top-level definitions, regardless if exported or not.
However, that leads to severe performance regressions, as GHC aggressively floats out local bindings to the top-level if possible. 
Call Arity, in particular, relies on the assumption that only exported identifiers are externally visible to achieve its good results.
The Demand Analyser, in contrast, goes with the conservative assumption that all top-level bindings are used.

The problems we faced are closely related to the problem GHC's Occurence Analyser tries to solve, but we refrained from mirroring even more unrelated logic into an already quite complex usage analysis.

\section{On `Interesting' Identifiers}\label{sec:int}

Call Arity utilises co-call graphs for its sharing analysis, which can be quite expensive, because of the inherent quadratic complexity. 
Although the graph data structure used for co-call graphs allows for efficient insertion, constructing the adjacency set of a node is quite costly.

That is why Call Arity tracks only `interesting' identifiers in its data structures, assuming conservative results for all other identifiers \parencite{callarity}. 
Identifiers which are deemed interesting have a function type and are locally \keyword{let}-bound.
This is good enough for the very specific purpose that Call Arity set out to optimize: 
Formulating the commonly used \hsinl{foldl} as a right fold without causing unnecessary allocation.

Except, we can't make the same assumptions when we also want to generalise the usage analysis within the Demand Analyser.
From a usage perspective, all bindings carry important usage information. 

Because of the same challenges regarding huge constructor applications outlined in \textcite[section~3.4.1]{callarity}, co-call graphs are the time and space bottleneck of our analysis.

While the previous co-call graph data structure is well-suited for small graphs, the representation as an unreduced union of complete and complete-bipartite graphs makes edge tests require time linear in the size of the union in the worst case. 
Let alone the unpredictable space usage, possibly exceeding quadratic complexity for the same reason.
Paired with the requirement imposed by fixed-point iteration to efficiently check co-call graphs for equality, we chose to revise the graph data structure to be represented in a more predictable reduced form, as explained in \cref{sec:graphrep}.

\section{Graph Representation}\label{sec:graphrep}

\Cref{sec:int} brought up performance issues regarding the data structure used to model graphs.

Call Arity necessitated an efficient way to handle either sparse or dense graphs.
\textcite{callarity} chose to represent graphs as a simple, unreduced union of complete and complete bipartite graphs, simply because it provided the right tradeoffs for small- to medium-sized graphs.

However, since we got rid of the notion `interesting' variables (\cf \cref{sec:int}), the performance issues resurfaced.

The unreduced union representation has problems when the union consists of many, small graphs:
Computing the adjacency set of a node, or even simply testing for an edge in a graph of constant size, may take time linear in the length of the union.
Space complexity is unpredictable in the same way:
Even for represented graphs of constant size, the size of the representation scales linearly in the length of the union.
Uniting a graph itself results in a graph of twice the size.
Such an operation is quite common in fixpointing, so exponential blowup is imminent.

More concretely, at one point through development, space usage exceeded sixteen gigabytes for some input files, bogging down the whole development system.

Hence, in order to have better guarantees about space and runtime complexity, we took inspiration in representing the common cases of sparse and dense graphs efficiently, while we made sure that edge tests were still efficient by storing the represented graph's node-indexed adjacency sets (witnessed by an isomorphism $\sGraph \simeq \sVar \to \sVar$ modulo symmetry) directly.

The representation either stores the edge set of complement graph or the graph's edge set directly.
Obviously, the former exhibits better performance characteristics for dense graphs, while the latter should be favored for sparse graphs.

Edge tests have become very cheap in the new data-structure, while the complexity hides in implementing the $\llub$ and $\both$ operators, which will need to flip between representations as appropriate.
When to flip is determined based on density of the graph:
If above a constant threshold greater than $\nicefrac{1}{2}$, the graph representation is to be flipped.

While this doesn't get rid of the potential quadratic space complexity, this tremendously helped in bringing down memory usage to more predictable figures.

\section{Bounding Product Uses}\label{sec:bound}

As pointed out in \cref{sec:fix}, we need to make sure to bound the depth of product uses in order for the domain of monotone usage transformers to satisfy the ascending chain condition, giving some guarantee of termination.

Otherwise, the mentioned infinitely ascending chain actually occurs for usage signatures of coinductive definitions. 
Such definitions are permitted in a lazy functional language like Haskell and can also be emulated in strict languages through explicitly delayed computations. 
Consider this snippet on lazy streams:

\begin{haskellcode}
  data IntStream = MkStream Int IntStream

  triple :: IntStream -> IntStream
  triple (MkStream x xs) = MkStream (3 * x) (triple xs)
\end{haskellcode}

Approximation of the usage transformer of \hsinl{triple} will begin with $\bot$. If put under use $U$, the usage on the first argument will ascend to $1*U(1*U, A)$, then to $1*U(1*U, 1*U(1*U, A))$ and so on.

We currently bound the depth of product uses to 10, which is quite arbitrary. 
Termination time, however, is affected exponentially by the cut-off depth.

\section{Approximating Usage Transformers}\label{sec:approx}

As we saw in \cref{sec:fix}, all denoting usage transformers are monotone, which is a necessary condition for termination of the analysis.

However, we can't just naively approximate a function.
At least we would need an appropriate data structure for monotone maps between lattices.
Then, we would also need some way of predicting the points where actual steps in the ascending chain happen, otherwise we would still need to compute every point.

However, we don't need to approximate every point of a usage transformer.
We can do much better by recognising that only ever finitely many (most of the time only one) points of a transformer are accessed!

After all, for the outermost \keyword{let} expression representing the module, the only usage type that we are interested in is that under use $U$.
To compute that usage type, we only ever access single points of a usage transformer, etc.

So, instead of approximating usage transformers at \emph{every possible point}, we employ a more demand-driven approach and approximate only those points we transitively access from the root expression under use $U$.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis lines = left,
        ylabel=$\theta$,
        ymin = 0.1,
        xtick = {0,0.5,1.5,2},
        xticklabels = {$HU$,$C^1(HU)$,$C^1(U)$,$U$},
        ytick = \empty,
      ]
      \addplot[domain=0:2, color=blue]{(x-1)^3 + 1};
      \addplot[domain=2:2.5, color=blue]{2};
      \addplot+[domain=0:0.5, color=red, const plot] coordinates {%
        (0,0) (0.5, 0.875) (1.5, 1.125) (2,2)
      };
    \end{axis}
  \end{tikzpicture}
  \caption{Sketch of a monotone usage transformer and a monotone approximation in four points. Maps a chain in $\sUse$ to one in $\sUType$.}
  \label{fig:approx}
\end{figure}

\Cref{fig:approx} shows a (for illustrative purposes continuous) usage transformer that is approximated in finitely many points. 
Note that the approximation is stable only in four points, but possibly unstable in all others.
That doesn't matter much, assuming that only the four stable points are ever accessed.\smallskip

This argument is comparable to the difference in evaluation order between dynamic programming and memoisation: 
Memoisation follows a demand-driven top-down scheme, whereas dynamic programming computes the solution from the bottom up.
When the solutions to all subproblems are really needed, both approaches perform the same work.

However consider the following artificial recurrence of a function for fixed naturals $c$ and $p$:
\[
  f(n, i) = \begin{cases}
    n+i, & \text{when } n = 0 \\
    i*f(n-1, i*c\tpmod{p}), & \text{otherwise}
  \end{cases}
\]

Aside from solving this recurrence in some smart way, let's compare how the different approaches would calculate an arbitrary point $f(n,i)$.

A reasonably efficient dynamic programming strategy would need to fill a tableau with $p*n$ entries, starting with the column $f(0,i)$ for all $i$.
In contrast, memoisation will just need to follow a single thread of $n$ points through the recurrence to compute $f(n,i)$\footnote{Granted, there are no overlapping sub-problems, so no caching is needed at all, but that could be changed by just adding one additional case to the recurrence.}.

To make matters worse, if we removed the modulo operator, we could not solve $f$ with a tableau at all!
This is the same situation with monotone usage transformers: 
Without knowing at which points the steps in the strictly ascending chain are made, we cannot compute the monotone map in finite time.
Also, a memoisation-based (or demand-driven, lazy, top-down, \dots) approach computes only those points we are interested in.

Of course, a recurrence like that of \cref{sec:transfer} generalises on problems suited to be solved by dynamic programming and memoisation, in that the definition of a point may directly or indirectly refer to itself (\eg, introducing cycles to the dependency graphs we solve).
In terms of a solution strategy, for a typical memoisation problem, it is sufficient to memoise already computed results in some kind of map. 
For a recurrence however, we need to propagate updates of unstable points, leading to the usual data-flow frameworks solved through fixed-point iteration.

\section{Annotations}

The abstract specification in \cref{sec:transfer} leaves out many details a real implementation must account for.

The most glaring simplification is that of returning analysis results. 
The specification describes how to interpret an expression abstractly with respect to usage, but forgets to actually announce its findings!

A real implementation would thus thread annotations as an additional output. 
In GHC it is common to thread the annotated expressions directly, without collecting analysis results in some kind of map first.
This is a little unfortunate, as a map would allow to efficiently check for changed annotations.
However, intermediate passes within GHC guarantee only that the unique keys of identifiers are unique \emph{within their scope}, so there might be clashes when merging the results of two different closed expressions.

Thus, our usage analysis annotates expressions directly (this has impliciations on change detection in \cref{sec:solve}) and in fact we annotate the same information as \textcite{card} does:

\begin{enumerate}
  \item When analysing lambda expressions, we mark the lambda as \emph{one-shot} if the incoming use is of the form $C^1(\uscore)$.
  \item We annotate any binder with the usage recorded in the usage type relative to incoming use. This applies to lambda binders (after we multiply body usage, but before we delete the binder from the usage type), \keyword{case} binders, data constructor fields and of course \keyword{let} binders (looked up after the fixed-point of $\up$ is reached, before we delete the binders of the group).
\end{enumerate}

Note that the annotations depend on the incoming use. 
This means annotations only make sense when the use is a conservative approximation to every possible use.
We see in \cref{sec:solve} an example where use on a call site produces too optimistic annotations, which of course may not be used.

If at one point an expression is absent (which happens only for \keyword{let} bound expression or arguments), we mark all binders in that expression as absent.

In order to enable more precise results across module boundaries, \textcite{dmd} provided \emph{demand signatures} for each exported function in the module's interface file. 
This was extended by \textcite{card} to usage signatures, and as such we also annotate each exported function with its usage signature.

Note that by the time we import a function, its free variable usage is uninteresting:
The environments would only mention free variables which were already compiled and whose usage was conservatively approximated because of unforeseeable use sites.
Hence the usage signature captures all important information.

Now, the usage signature of an expression alone has no semantic meaning without the use it was produces under.
The annotated usage signature is actually a digest of the full usage transformer, approximated at three prominent points.
From a usage signature for an incoming call use corresponding to the arity $\alpha$ (as computed by an arity analysis) of the expression, we can derive the following usage transformer:
\[
  \tau_x~u = \begin{cases}
    \bot, & \text{when } u \lless \underbrace{C^1(C^1(\ldots C^1(\uscore)\ldots))}_{\alpha~\text{times}} \\
    \lTriple{\emptyset}{\emptymap}{\sigma}, & \text{when } u \lleq \underbrace{C^1(C^1(\ldots C^1(U)\ldots))}_{\alpha~\text{times}} \\
    \lTriple{\emptyset}{\emptymap}{\omega*\sigma}, & \text{otherwise}
  \end{cases}
\]

This is really similar to the treatment of product constructors (\cf $\tau_{\sMkPair}$ in \cref{sec:var}), except that there is no polymorphism in regard to the product use of the call.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis lines = left,
        ylabel=$\theta$,
        xtick = {0,1.5,2},
        xticklabels = {$HU$,$u$,$U$},
        ytick = \empty,
      ]
      \addplot[domain=0:2, color=blue]{(x-1)^3 + 1};
      \addplot[domain=2:2.5, color=blue]{2};
      \addplot+[color=red, const plot mark right] coordinates {%
        (0,0) (1.5, 1.125) (2,2)
      };
    \end{axis}
  \end{tikzpicture}
  \caption{Sketch of a usage transformer derived from a usage signature for a single incoming call use $u$.}
  \label{fig:sigtrans}
\end{figure}

\Cref{fig:sigtrans} shows a sketch mapping a chain in $\sUse$ to one in $\sUType$ in the spirit of \cref{fig:approx}.
In contrast to the situation for fixed-point iteration, where we start with optimistic approximations, the digested usage transformer is conservative, \eg approximates from above.

It also shows how our usage analysis generalises on the \letdnsc rule in \textcite{card}:
Their approach is to always interpolate the usage transformer in those three points, even for local bindings.
This guarantees at most one pass over an expression, disregarding fixed-point iteration.
We can recover the same results by modifying the $\letdown{\uscore}{\uscore}$ combinator to simplify all incoming call uses to one of the three cases.

Since we wanted to generalise on Call Arity, at least arbitrary call demands should be permitted (\eg truncating products mentioned in incoming call uses to $U$).
We will see the implications on compiler performance and performance of the produced artifact in \cref{sec:eval}.

\section{Solving the Data-flow Problem}\label{sec:solve}

Analyses within GHC are commonly guided by the structure of the syntax tree.
This is attractive from the point of view of simplicity:
An analysis is just a fold over the expression, returning the annotated expression.

However, for our analysis and with the approach from the last section, we encounter a problem at bindings like \hsinl{let f x = x in e}: 
How can we know at which uses we need to calculate the usage transformer of \hsinl{f} without looking at \hsinl{e}?
We know for sure at the call sites of \hsinl{f}, so we can just approximate its usage transformer on demand \emph{while analysing} \hsinl{e} and memoise it through laziness.

This gets complicated really quickly if we add recursive binding groups:
\begin{haskellcode}
  let fac n = 
        if n == 1 
        then 1 
        else n * fac (n-1)
  in fac 12
\end{haskellcode}

When we hit the call to \hsinl{fac} in the body, we have to query the approximated usage transformer for a stable value at the incoming call use.
For this, we have to perform fixed-point iteration, assuming a very optimistic $\bot$ usage transformer (or a prior approximation) for \hsinl{fac} and iterate until the usage type for the requested use doesn't change any more.
This means we have to flag any reference to unstable points of the usage transformer.

Hinting at mutual recursion and recursive calls in different uses, it should have become clear that going down this route quickly becomes too complex to follow and implement. 
That is why analyses like the Demand Analyser deliberately settled for analysing bound expressions before the body and vice versa, embodied in the \letdnsc and \letupsc rules, respectively. 
These were discussed in \textcite[section~3.5--3.6]{card} and \cref{sec:let}.

Another drawback is that techniques which are concerned with computing fixed-points, but are otherwise orthogonal to the analysis implemented, are intertwined with the analysis logic.
Analysis order depending on the syntactic structure is just a minor example of this. 
A less simple technique is that of caching of analysis results described in \textcite[section~9.2]{dmd}. 

All these approaches are present in every analysis within GHC (or they would benefit from them, at least) and every analysis for itself may get it wrong in a particular way, let alone the increase in cognitive complexity.

Looking at compilers for imperative programming languages, data-flow problems are solved by fixed-point iteration over the control flow graph (or over a graph-based intermediate representation \parencite{firm}, \parencite{thorin}) and is broken down into two parts.

An \emph{iteration strategy} determines which nodes in the data-flow graph are still unstable and need to be recomputed, and the order in which to do so. 
The chosen strategy is completely opaque to the data-flow framework to solve, but severely impacts the time it takes to arrive at a stable solution.
Commonly, the nodes which need to be updated are tracked within a \emph{worklist} in order not to update currently stable nodes.
The order in which the worklist processes unstable nodes also affects performance \parencite{dfa}.
The caching of analysis results between iterations \parencite{dmd} is a nice side-effect of the explicit graph abstraction.

Additionally, associated with each node is a \emph{transfer functions} which has the single responsibility of recomputing analysis information in terms of the current state of its dependencies.

Driven by the looming complexity in analysis order, we decided to break down our analysis in a similar manner.
In an effort to specify transfer functions separate from the iteration strategy, we arrived at the (slightly simplified) interface in \cref{fig:worklist}.

\begin{figure}[h]
  \centering
  \begin{haskellcode}
    runFramework 
      :: Ord node
      -> DataFlowFramework node lattice 
      -> Set node 
      -> Map node lattice
    runFramework = ...

    data DataFlowFramework node lattice = DFF 
          (node -> TransferFunction node lattice lattice)
          (node -> ChangeDetector node lattice)

    type ChangeDetector node lattice
      = Set node -> lattice -> lattice -> Bool

    data TransferFunction node lattice a
      = TFM (State (WorklistState node lattice) a) -- not exported!
      deriving (Functor, Applicative, Monad)

    dependOn
      :: Ord node 
      => node 
      -> TransferFunction node lattice (Maybe lattice)
    dependOn
  \end{haskellcode}
  \caption{The essence of the \texttt{utils/Worklist.hs} module.}
  \label{fig:worklist}
\end{figure}

A \hsinl{DataFlowFramework} assigns to each abstract \hsinl{node} a \hsinl{TransferFunction} and a \hsinl{ChangeDetector} that compares the old value with the updated value to detect when the node has become stable. 
For reasons becoming clear later on, the \hsinl{ChangeDetector} is also supplied the set of referenced \hsinl{node}s that changed since the last iteration.

Although we purposefully named the type variable denoting the analysis domain \hsinl{lattice}, we don't actually require it to model any kind of algebraic structure.
This is something we come back to in \cref{sec:mono}.

The first two type parameters of \hsinl{TransferFunction} specify in what kind of data-flow framework the function operates, while the last parameter corresponds to the return value of that \hsinl{TransferFunction}.
Talking about what a transfer function `returns' might not make much sense just yet, but the derived \hsinl{Monad} instance hints at how this will turn out.
As always, a \hsinl{Monad} instance is useful to weave effects into otherwise pure computations. 
We can see that \hsinl{TransferFunction} is just an opaque wrapper around a stateful computation, but have no way of conjuring such a side-effect apart from cheating our way in with \hsinl{pure}.

The single means for introducing a `side-effect' is through \hsinl{dependOn}, which announces an edge in the data-flow graph by referencing the value of another node.
Depending on whether the iteration strategy can provide a value (at least cycles in the graph have to be broken at some point), it may or may not return a value, in which case the \hsinl{TransferFunction} is obliged to continue with an optimistic approximation (\eg $\bot$).

Note that just by executing the \hsinl{TransferFunction} in another \hsinl{WorklistState}, we can iterate a transfer function in a completely isolated manner.
The iteration strategy decides if the value of a node is immediately to be recomputed in a call to \hsinl{dependOn}, or if a value from a prior iteration is to be returned, or none at all.

The \hsinl{Ord} instance on \hsinl{node}, apart from being necessary for use as key in a \hsinl{Map}, serves as a priority on the nodes in the worklist.
By choosing a total order that mirrors how an analysis would proceed along the syntax tree, we are promised fast convergence by GHC's Occurence Analyser which arranged bindings in a suitable order.

In practice, we modeled each single point of a usage transformer as a separate node. 
This resulted in a specialisation of \hsinl{node} to \hsinl{(FrameworkNode, Use)}\footnote{Of course, the total order on \hsinl{Use} is not actually compatible with the join-semilattice we usually refer to and doesn't have any semantic meaning.}, where the total order on \hsinl{FrameworkNode} mirrors the syntax tree, and \hsinl{lattice} to \hsinl{(UsageType, CoreExpr)}, where the returned \hsinl{CoreExpr} is annotated with the findings of the analysis.
As it turned out, we lost important structure in forgetting about monotonicity (\cf \cref{sec:mono}).\smallskip

Allocating \hsinl{FrameworkNode}s for every syntactic element enables caching of intermediate results and avoids whole chains of nodes to be recomputed when no change is detected.
On the other hand, the bookkeeping in the iteration algorithm might outweigh any performance benefits of caching.
Also, memory usage becomes an issue for big graphs.
This is why we decided to only allocate \hsinl{FrameworkNode}s where we needed to break cycles in the data-flow graph.
Cycles arise exactly where we used the \fix operator to tie knots in \cref{sec:letrec}, thus at least we need to allocate nodes for the right-hand sides of bindings (referred to as \letdnsc nodes) and for whole \keyword{let} bindings (\letupsc nodes).

Of course, just by allocating nodes the feedback cycles aren't broken yet:
We need to detect when an iteration of a node does not change anymore. 
For usage types, detecting change is pretty standard by delegating to the derived \hsinl{Eq} and can be sped up considerably by exploiting monotonicity.
As an example, we can avoid potentially quadratic time comparison of co-call graphs by just checking if the number of total edges changed.

Detecting changes in the annotated syntax tree isn't so cheap. 
In fact, traversing entire expressions is infeasible for huge modules from a performance perspective.
We can rely on another convenient fact, though:
Annotated expressions only depend on subexpressions and themselves don't introduce dependency cycles.
This means that when the only referenced node that changed was the current node itself, there was no change to the annotated expression.

That is why \hsinl{ChangeDetector}s are supplied the set of changed references:
Apart from checking usage types for changes, it checks if the only reference that changed was the node itself.

Other than that, we have to allocate a node (\hsinl{root} in the example that follows) for the module expression to have a way to refer to it and then kick off iteration with a call to \hsinl{runFramework} like the following:
\begin{haskellcode}
  result :: Map (FrameworkNode, Use) (UsageType, CoreExpr)
  result = runFramework framework (Set.singleton (root, topUse))
\end{haskellcode}

By passing the singleton set as the second argument, we express that the module expression is put under top use $U$.
From there, the iteration algorithm begins to explore the data-flow graph in a depth-first fashion, which -- as we already pointed out at the begin of this section -- is crucial for termination, as the graph is infinite, while the nodes reachable from $(\hsinl{root}, U)$ is finite.

This is best understood by a simple example which already exhibits quite a complex iteration order.

\begin{example}
  Consider the following example program, printing the factorial of 12:
  \begin{haskellcode}
    module Main (main) where

    fac n = 
      if n <= 1 
      then 1 
      else n * fac (n - 1)

    main = print (fac 12)
  \end{haskellcode}

  This will be translated to the following module expression:
  \begin{haskellcode}
    let fac n =
          if n <= 1
          then 1
          else n * fac (n - 1)
    in let main = print (fac 12)
       in (main) -- This is a 1-tuple, \eg introduces a box
  \end{haskellcode}

  \Cref{fig:framework} depicts the resulting data-flow framework, or rather the finite part reachable from node $(\hsinl{root}, U)$ (again, \hsinl{root} represents the usage transformer denoting the module expression). 

  \begin{figure}[h]
    \centering
    \begin{tikzpicture}[shorten >=1pt,x=7em,y=7em,auto]
      \tikzset{every loop/.style={looseness=4}}
      \tikzstyle{n}=[draw,shape=circle,minimum size=5em,inner sep=0pt]
      \tikzstyle{root}=[n,fill=myred]
      \tikzstyle{up}=[n,fill=myblue]
      \tikzstyle{down}=[n,fill=mygreen]
      \node (root) at (0,2) [root] {$(\hsinl{root}, U)$};
      \node (let1) at (1,2) [up] {$(\keyword{let}_1, U)$};
      \node (let2) at (2,2) [up] {$(\keyword{let}_2, U)$};
      \node (main) at (2,1) [down] {$(\hsinl{main}, U)$};
      \node (fac1) at (1.5,0) [down] {$(\hsinl{fac}, C^1(U))$};
      \node (facn) at (1,1) [down] {$(\hsinl{fac}, U)$};
      \path[->,line width=1pt] 
        (root) edge node {} (let1)
        (let1) edge [loop above] node {} ()
               edge node {} (let2)
               edge node {} (facn)
        (let2) edge node {} (main)
        (main) edge node {} (fac1)
        (facn) edge node {} (fac1)
        (fac1) edge [loop below] node {} ();
    \end{tikzpicture}
    \caption{Relevant part of the data-flow framework for the \hsinl{fac} example. The module node is drawn in red, \letupsc nodes are blue and \letdnsc nodes are green.}
    \label{fig:framework}
  \end{figure}

  \todo{Maybe annotate the graph with numbers for the steps we are in}

  The iteration algorithm will start with iterating the \hsinl{TransferFunction} of the module expression, represented by the red node, under use $U$ and then proceed in the following order:\footnote{Note that we effectively \hsinl{uncurry} our transfer function from \cref{sec:transfer}, turning something of type $\sExp \to \sUse \to \sUType$ into something of type $\sExp \times \sUse \to \sUType$. Thus, \hsinl{TransferFunction} transfers expressions into the domain of usage types instead of usage transformers (see \cref{sec:mono} on impliciations for monotonicity).}

  \begin{enumerate}
    \item 
      The transfer function associated with $(\hsinl{root}, U)$ forwards to the \letupsc node of the \keyword{let} expression binding \hsinl{fac} in the same use, $(\keyword{let}_1, U)$ in blue.
    \item 
      The depth-first strategy immediately descends into said \letupsc node.
      Since the binding for \hsinl{fac} is recursive, the \letupsc node \hsinl{dependsOn} itself, for usage of \hsinl{fac} in the last iteration.
      This is the first iteration, so \hsinl{dependOn} returns \hsinl{Nothing} and the usage in the body serves as a first approximation.
    \item 
      The body of the outer \keyword{let} is the inner \keyword{let} binding for \hsinl{main}, represented by another blue node $(\keyword{let}_2, U)$.
      Since \hsinl{main} is non-recursive, looking at usages in the body is sufficient.
    \item The incoming use $U$ translates into a use of $U \equiv U(\omega*U)$ on the implied 1-tuple \hsinl{main}, which causes a dependency on the \letdnsc node of \hsinl{main} in use $U$ (\letdnsc nodes are green).
    \item 
      We immediately descend into said \letdnsc node, where the use $U$, through \hsinl{print}, puts \hsinl{fac} under a call use $C^1(U)$. 
    \item 
      After descending into (`calling') the \letdnsc node \hsinl{fac}, the analysis tries to recurse into $(\hsinl{fac}, C^1(U))$. 
      This cycle is broken by returning \hsinl{Nothing} from \hsinl{dependOn}; the analysis will compensate for that by unleashing a usage type of $\bot$ at the call site.
      The result is a (first, approximate) usage type of $\lTriple{\emptyset}{\maplit{\hsinl{fac}}{C^1(U)}}{1*U \to \top}$ for $(\hsinl{fac}, C^1(U))$.
      (An irrelevant fact, because we don't use the annotated expression: The argument to the bound expression is annotated too optimistically as being single-entry and the lambda as one-shot.)
    \item
      Analysis of the expression bound to \hsinl{main} continues.
      The call use on $\hsinl{fac}$ gets sequentially combined with the call use from the unleashed \letdnsc node, for a total call use of $C^\omega(U) \equiv U$.
      Nothing iteresting happens to the literal $12$, thus the usage type of the \letdnsc node for $(\hsinl{main}, U)$ is $\lTriple{\{(\hsinl{fac}, \hsinl{fac})\}}{\maplit{\hsinl{fac}}{U}}{\top}$.
      However, because \hsinl{main}'s bound expression is not in WHNF, only the uninteresting usage signature is propagated to the call site within $(\keyword{let}_2, U)$.
      Note that the annotated expression would be calculated at this point, too. 
      We don't need it at the call site, but when we handle the \keyword{let} binding in the next step.
    \item
      Unwinding the call stack once more, the inner \keyword{let} binding for \hsinl{main} is now resolved as part of the \letupsc node $(\keyword{let}_2, U)$.
      The usage \hsinl{main} is exposed to in the body is $\omega*U$.
      Thus, there is a dependency on the \letdnsc node $(\hsinl{main},U)$, at least for the annotated expression.
      The algorithm just iterated that node in the last step, so its result is reused.
      Since the expression bound to \hsinl{main} is not in WHNF, we also unleash the associated usage type, resulting in a usage type of $\lTriple{\{(\hsinl{fac}, \hsinl{fac})\}}{\maplit{\hsinl{fac}}{U}}{\top}$ for the whole inner \keyword{let} expression.
    \item
      Another unwind resumes analysis in the \letupsc node $(\keyword{let}_1, U)$, the binding for \hsinl{fac}.
      The body exposes \hsinl{fac} to a usage of $\omega*U$, even before sequentially composition with itself induced by the recursive \keyword{let} case.
      Although the right-hand side of \hsinl{fac} is in WHNF (so usage types have been unleashed at call sites), we still need the annotated expression under use $U$.
      This introduces a dependency on the \letdnsc node $(\hsinl{fac}, U)$ for which there is no value yet, causing the algorithm to do a `call'.
    \item
      Analysis of \hsinl{fac} under use $U$ yields the same usage type as under use $C^1(U)$, but that is quite irrelevant.
      More importantly, the lambda is not one-shot and the argument binder for \hsinl{n} is not single-entry, contrary to the optimistic situation under use $C^1(U)$.
      Also, $(\hsinl{fac}, U)$ is not `recursive', rather it depends on $(\hsinl{fac}, C^1(U))$.
    \item
      Unwinding to $(\keyword{let}_1, U)$ again, this results in an uninteresting usage type $\lTriple{\emptyset}{\emptymap}{\top}$ for the annotated expression.
    \item
      Finally, analysis proceeds in the top-level $(\hsinl{root}, U)$ node, which just forwards the results from $(\keyword{let}_1, U)$.
    \item 
      Now the actual worklist algorithm takes over:
      While computing the current approximation, we were using unstable results (\eg where \hsinl{dependOn} returned \hsinl{Nothing}) for $(\hsinl{fac}, C^1(U))$ and $(\keyword{let}_1, U)$.
    \item
      $(\hsinl{fac}, C^1(U))$ has the higher priority, so it is iterated first.
      This results in a more precise usage type of $\lTriple{\{\hsinl{fac}, \hsinl{fac}\}}{\maplit{\hsinl{fac}}{U}}{\top}$.
      The change marks the referrers $(\hsinl{fac}, C^1(U))$, $(\hsinl{fac}, U)$ and $(\hsinl{main}, U)$unstable.
    \item
      After one more iteration, $(\hsinl{fac}, C^1(U))$ is deemed stable.
      Although irrelevant, the annotated expression stayed the same.
    \item
      Next highest priority node is $(\hsinl{fac}, U)$, where the change on $(\hsinl{fac}, C^1(U))$ yields the same usage type (which was not used) and the same annotations.
      The change in usage type marks its referrer $(\keyword{let}_1, U)$ as unstable once more.
    \item
      In $(\hsinl{main}, U)$, the changes in $(\hsinl{fac}, C^1(U))$ did not make a difference at all, so the node is still stable.
      Hence, no need to reiterate $(\keyword{let}_2, U)$.
    \item
      The only unstable node left is $(\keyword{let}_1, U)$.
      The usage type from the last iteration is unchanged, but the annotations in \hsinl{fac} changed. 
      So its referrers $(\keyword{let}_1, U)$ and $(\hsinl{root}, U)$ are marked as unstable.
    \item
      Another iteration on $(\keyword{let}_1, U)$ reveals no further change.
    \item
      Finally, the $(\hsinl{root}, U)$ node is iterated and marked as changed because of annotations in sub-expressions.
      That however does not mark any referrer as unstable, since there are none.
    \item
      The algorithm returns current stable graph as a map from nodes to results.
  \end{enumerate}

  The infiniteness of the graph surfaces at the two different nodes for \hsinl{fac}:
  Actually, there are many more of these nodes, but only the two points for $C^1(U)$ and $U$ are reachable from $(\hsinl{root}, U)$.
\end{example}

\section{Monotonicity}\label{sec:mono}

As we outlined in \cref{sec:letrec}, monotonicity of all involved usage transformers is essential in proving existence of the fixed-point.
\Cref{sec:approx} pointed out that approximating usage transformers in finite time is still impossible without restricting interest to a finite set of points.

This lead to a data-flow framework in \cref{sec:solve} where we modeled each single point of a usage transformer as a separate node.
We argued that this is similar to \hsinl{uncurry}ing the transfer function in \cref{sec:transfer} from $\sExp \to \sUTrans \equiv \sExp \to \sUse \to \sUType$ to $\sExp \times \sUse \to \sUType$.
The problem with doing so is that it doesn't preserve the monotonicity of denoting usage transformers.

To be more precise, the transfer function is more accurately described by the type $\sExp \to (\sUse \mto \sUTrans)$, where $\mto$ denotes a monotone map.

Where does this bite?
Well, we relied on monotonicity in our argument for proving existence of the fixed-point.
Of course, \hsinl{uncurry}ing didn't change the actual semantics, but modeling each point separately means that prior to reaching the fixed-point, there are unstable intermediate approximations which are not monotone.
It turns out that convergence depends even on these unstable approximations to be monotone, because there are expressions which diverge otherwise:

\todo{Reproduction}

\section{Hacking on GHC}

Working on a central part of a `real-world' compiler such as GHC was challenging in ways beyond thinking about combining two analyses on a drawing table.

Usage information is critical to many other Core-to-Core passes within GHC\@.
More subtle bugs require entire days of tracing through tests and thinking hard for minimal reproductions in order to better understand the problem.

Many bugs hid behind module boundaries, because the code concerned with serialising usage signatures is quite scattered.
Some identifiers, such as dictionary selectors, primitive operators and runtime errors, get special treatment by GHC, the places at which this happens were discovered in a number of successive debugging sessions.

Not-so-absent thunks which the analysis identified as absent lead to crashes at runtime, only more informative than a segfault in that the message mentions absence as a reason.

So it came that the most gnarly class of bugs manifested themselves as absent errors which only popped up across module boundaries, involving type class instances referencing absent thunks.
It took quite some time and head-scratching to nail down rewrite rules as the culprit, which had to be regarded as reachability roots as explained in \cref{sec:toplvl}.

GHC's build system was another thing that took some time to get accustomed to.
Especially figuring out which things needed to be rebuilt after some change and what various build settings did, took a lot of trial and error.
Sometimes even a \texttt{make clean} wouldn't get rid of some clearly build system-related issues, leaving no choice but to do a complete fresh checkout.
Experiences like the latter don't exactly strengthen confidence in the build system, so we look forward to see \texttt{hadrian}\footnote{\url{https://github.com/snowleopard/hadrian}} succeed.

Lastly, the test and benchmark suites of GHC are quite essential in immediately crushing occasional hopes after having fixed a complicated bug by immediately confronting the developer with another regression.
Of course, this is a good thing for both the maturity of GHC as well as a humbling experience for the soul.
