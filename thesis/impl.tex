\chapter{Implementation}\label{sec:impl}

This chapter is concerned with the implementation of a usage analysis as specified in \cref{sec:spec} within the Glasgow Haskell Compiler (GHC).

A detailed walkthrough of the Haskell code is out of scope and would not be particularly interesting, so we will instead discuss design decisions and interesting problems we encountered.

\section{Object Language}

After a Haskell program passes through GHC's frontend, it is compiled down to an explicitly typed core calculus called GHC Core. 
Core is the first of a number of intermediate languages the program is translated to before an executable artifact is produced.
Already being vastly simpler than Haskell's huge surface syntax, as can be seen in \cref{fig:core}, Core is still quite complex compared to the object language introduced in \cref{sec:exp}.

While Core is still unconcerned with operational details, most optimisations within GHC are realised as Core-to-Core passes.
A non-strict, high-level language like Haskell provides ample opportunities for optimisation even in this macroscopic context.
The representation as a lambda calculus allows simplification based on term rewriting, which GHC makes great use of in its simplifier.
Functional languages encourage composition of concise definitions, so GHC supports its optimisations with an aggressive inliner.

Apart from the simplifier, GHC employs other transformations which rely on precise information made available by analyses like GHC's Demand Analyser and Call Arity.
While the Demand Analyser combines strictness analysis \parencite{dmd} with a usage analysis \parencite{card} and constructed product result analysis \parencite{cpr}, Call Arity is an arity analysis interleaved with a sharing analysis based on co-call graphs, to find out which bindings can be $\eta$-expanded without losing any shared work \tod{move this elsewhere?}.

As is the case for Demand Analysis and Call Arity, our usage analysis will operate on and annotate GHC Core expressions.

\begin{figure}[h]
  \begin{haskellcode}
    data Expr b
      = Var      Id
      | Lit      Literal
      | App      (Expr b) (Expr b)
      | Lam      b (Expr b)
      | Let      (Bind b) (Expr b)
      | Case     (Expr b) b Type [Alt b]
      | Cast     (Expr b) Coercion
      | Tick     (Tickish Id) (Expr b)
      | Type     Type
      | Coercion Coercion

    type Alt b = (AltCon, [b], Expr b)

    data AltCon
      = DataAlt DataCon
      | LitAlt  Literal
      | DEFAULT 

    data Bind b 
      = NonRec b (Expr b)
      | Rec [(b, (Expr b))]
  \end{haskellcode}
  \caption{Part of the data types representing the syntax of GHC Core}
  \label{fig:core}
\end{figure}

\section{Top-level Bindings}

The code of a module in GHC Core is represented as a list of top-level definitions, some of which are exported.

To avoid duplication with the treatment of \keyword{let} bindings, we translate the list of definitions into an expression of nested \keyword{let}s before the analysis and back after the analysis.

Which usage are top-level bindings exposed to? 
For exported bindings, we don't oversee all potential use sites, so we have to be conservative and assume $\omega*U$. 
Exported bindings are similar to garbage collection roots: 
All non-absent bindings must be reachable through an exported binding. 
This is because for non-exported top-level bindings, their whole scope is known.

Based on this observation, it is also clear what the expression within the innermost \keyword{let} should be: A tuple of the exported identifiers.
This encoding of modules is common-place in languages like JavaScript (by the name of \emph{Revealing module pattern}) that lack(-ed) a proper module system.

Considering exported identifiers as roots is necessary, but, as it turned out, not sufficient.
GHC's rewrite rules and vectorisation declarations possibly mention identifiers which are neither exported, nor reachable otherwise, so these must be included in the root set.

A similar problem occurs for \emph{unfoldings}. 
Unfoldings enable inlining across module boundaries by serialising the \emph{unoptimised} bound expression into the module's interface file, a Haskell-specific compilation artifact like object files.
These unfoldings play a crucial role in revealing opportunities for custom rewrite rules.

Because unfoldings consist of the unoptimized bound expressions, they potentially reference bindings which are already optimised away or replaced by an optimised variant in the actual object code. 
As for rewrite rules and vectorisation declarations, ignoring unfoldings can result in surprising behavior and unforeseen crashes due to execution of supposedly absent code.

Correct handling of unfoldings would require to treat them as alternative right-hand sides of the binding they decorate.
Experimental support for unfoldings in the style of \hsinl{if True then rhs else unfolding} resulted in scoping issues of inner bindings, as well as distortions of analysis results.
As we didn't observe any crashes related to unfoldings when compiling and running the entire compiler, test suite and benchmark suite, we postponed proper handling of the problem.

Of course, the simplest sufficient root set would be to include all top-level definitions, regardless if exported or not.
However, that leads to severe performance regressions, as GHC aggressively floats out local bindings to the top-level if possible. 
Call Arity, in particular, relies on the assumption that only exported identifiers are externally visible to achieve its good results.
The Demand Analyser, in contrast, goes with the conservative assumption that all top-level bindings are used.

The problems we faced are closely related to the problem GHC's Occurence Analyser tries to solve, but we refrained from mirroring even more unrelated logic into an already quite complex usage analysis.

\section{On `Interesting' Identifiers}

Call Arity utilises co-call graphs for its sharing analysis, which can be quite expensive, because of the inherent quadratic complexity. 
Although the graph data structure used for co-call graphs allows for efficient insertion, constructing the adjacency set of a node is quite costly.

That is why Call Arity tracks only `interesting' identifiers in its data structures, assuming conservative results for all other identifiers \parencite{callarity}. 
Identifiers which are deemed interesting have a function type and are locally \keyword{let}-bound.
This is good enough for the very specific purpose that Call Arity set out to optimize: 
Formulating the commonly used \hsinl{foldl} as a right fold without causing unnecessary allocation.

Except, we can't make the same assumptions when we also want to generalise the usage analysis within the Demand Analyser.
From a usage perspective, all bindings carry important usage information. 

Because of the same challenges regarding huge constructor applications outlined in \textcite[section~3.4.1]{callarity}, co-call graphs are the time and space bottleneck of our analysis.

While the previous co-call graph data structure is well-suited for small graphs, the representation as an unreduced union of complete and complete-bipartite graphs makes edge tests require time linear in the size of the union in the worst case. 
Let alone the unpredictable space usage, possibly exceeding quadratic complexity for the same reason.
Paired with the requirement imposed by fixed-point iteration to efficiently check co-call graphs for equality, we chose to revise the graph data structure to be represented in a more predictable reduced form, as explained in \cref{sec:graphrep}.

\section{Bounding Product Uses}\label{sec:bound}

As pointed out in \cref{sec:fix}, we need to make sure to bound the depth of product uses in order for the domain of monotone usage transformers to satisfy the ascending chain condition, giving some guarantee of termination.

Otherwise, the mentioned infinitely ascending chain actually occurs for usage signatures of coinductive definitions. 
Such definitions are permitted in a lazy functional language like Haskell and can also be emulated in strict languages through explicitly delayed computations. 
Consider this snippet on lazy streams:

\begin{haskellcode}
  data IntStream = MkStream Int IntStream

  triple :: IntStream -> IntStream
  triple (MkStream x xs) = MkStream (3 * x) (triple xs)
\end{haskellcode}

Approximation of the usage transformer of \hsinl{triple} will begin with $\bot$. If put under use $U$, the usage on the first argument will ascend to $1*U(1*U, A)$, then to $1*U(1*U, 1*U(1*U, A))$ and so on.

We currently bound the depth of product uses to 10, which is quite arbitrary. 
Termination time, however, is affected exponentially by the cut-off depth.

\section{Approximating Usage Transformers}\label{sec:approx}

As we saw in \cref{sec:fix}, all denoting usage transformers are monotone, which is a necessary condition for termination of the analysis.

However, we can't just naively approximate a function.
At least we would need an appropriate data structure for monotone maps between lattices.
Then, we would also need some way of predicting the points where actual steps in the ascending chain happen, otherwise we would still need to compute every point.

However, we don't need to approximate every point of a usage transformer.
We can do much better by recognising that only ever finitely many (most of the time only one) points of a transformer are accessed!

After all, for the outermost \keyword{let} expression representing the module, the only usage type that we are interested in is that under use $U$.
To compute that usage type, we only ever access single points of a usage transformer, etc.

So, instead of approximating usage transformers at \emph{every possible point}, we employ a more demand-driven approach and approximate only those points we transitively access from the root expression under use $U$.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis lines = left,
        ylabel=$\theta$,
        xtick = {0,0.5,1.5,2},
        xticklabels = {$A$,$C^1(HU)$,$C^1(U)$,$U$},
        ytick = \empty,
      ]
      \addplot[domain=0:2, color=blue]{(x-1)^3 + 1};
      \addplot[domain=2:2.5, color=blue]{2};
      \addplot+[domain=0:0.5, color=red, const plot] coordinates {
        (0,0) (0.5, 0.875) (1.5, 1.125) (2,2)
      };
    \end{axis}
  \end{tikzpicture}
  \caption{Sketch of a monotone usage transformer and a monotone approximation in four points. Maps a chain in $\sUse$ to one in $\sUType$.}
  \label{fig:approx}
\end{figure}

\Cref{fig:approx} shows a (for illustrative purposes continuous) usage transformer that is approximated in finitely many points. 
Note that the approximation is stable only in four points, but possibly unstable in all others.
That doesn't matter much, assuming that only the four stable points are ever accessed.\smallskip

This argument is comparable to the difference in evaluation order between dynamic programming and memoisation: 
Memoisation follows a demand-driven top-down scheme, whereas dynamic programming computes the solution from the bottom up.
When the solutions to all subproblems are really needed, both approaches perform the same work.

However consider the following artificial recurrence of a function for fixed naturals $c$ and $p$:
\[
  f(n, i) = \begin{cases}
    n+i, & \text{when } n = 0 \\
    i*f(n-1, i*c\tpmod{p}), & \text{otherwise}
  \end{cases}
\]

Aside from solving this recurrence in some smart way, let's compare how the different approaches would calculate an arbitrary point $f(n,i)$.

A reasonably efficient dynamic programming strategy would need to fill a tableau with $p*n$ entries, starting with the column $f(0,i)$ for all $i$.
In contrast, memoisation will just need to follow a single thread of $n$ points through the recurrence to compute $f(n,i)$\footnote{Granted, there are no overlapping sub-problems, so no caching is needed at all, but that could be changed by just adding one additional case to the recurrence.}.

To make matters worse, if we removed the modulo operator, we could not solve $f$ with a tableau at all!
This is the same situation with monotone usage transformers: 
Without knowing at which points the steps in the strictly ascending chain are made, we cannot compute the monotone map in finite time.
Also, a memoisation-based (or demand-driven, lazy, top-down, ...) approach computes only those points we are interested in.

Of course, a recurrence like that of \cref{sec:transfer} generalises on problems suited to be solved by dynamic programming and memoisation, in that the definition of a point may directly or indirectly refer to itself (\eg, introducing cycles to the dependency graphs we solve).
In terms of a solution strategy, for a typical memoisation problem, it is sufficient to memoise already computed results in some kind of map. 
For a recurrence however, we need to propagate updates of unstable points, leading to the usual graph-based fixed-point iteration techniques such as the worklist algorithm.

\section{Solving Data-flow Problems}

Analyses within GHC are commonly guided by the structure of the syntax tree.
This is attractive from the point of view of simplicity:
An analysis is just a fold over the expression, returning the annotated expression.

However, for our analysis and with the approach from the last section, we encounter a problem at bindings like \hsinl{let f x = x in e}: 
How can we know at which uses we need to calculate the usage transformer of \hsinl{f} without looking at \hsinl{e}?
We know for sure at the call sites of \hsinl{f}, so we can just approximate its usage transformer on demand \emph{while analysing} \hsinl{e} and memoise it through laziness.

This gets complicated really quickly if we add recursive binding groups:
\begin{haskellcode}
  let fac n = 
        if n == 1 
        then 1 
        else n * fac (n-1)
  in fac 12
\end{haskellcode}

When we hit the call to \hsinl{fac} in the body, we have to query the approximated usage transformer for a stable value at the incoming call use.
For this, we have to perform fixed-point iteration, assuming a very optimistic $\bot$ usage transformer (or a prior approximation) for \hsinl{fac} and iterate until the usage type for the requested use doesn't change any more.
This means we have to flag any reference to unstable points of the usage transformer.

Hinting at mutual recursion and recursive calls in different uses, it should have become clear that going down this route quickly becomes too complex to follow and implement. 
That is why analyses like the Demand Analyser deliberately settled for analysing bound expressions before the body and vice versa, embodied in the \letdnsc and \letupsc rules, respectively. 
These were discussed in \textcite[section~3.5--3.6]{card} and \cref{sec:let}.

Another drawback is that techniques which are concerned with computing fixed-points, but are otherwise orthogonal to the analysis implemented, are intertwined with the analysis logic.
Analysis order depending on the syntactic structure is just a minor example of this. 
A less simple technique is that of caching of analysis results described in \textcite[section~9.2]{dmd}. 

All these approaches are present in every analysis within GHC (or they would benefit from them, at least) and every analysis for itself may get it wrong in a particular way, let alone the increase in cognitive complexity.

Looking at compilers for imperative programming languages, data-flow problems are solved by fixed-point iteration over the control flow graph (or over a graph-based intermediate representation \parencite{firm}, \parencite{thorium}) and is broken down into two parts.

An \emph{iteration strategy} determines which nodes in the data-flow graph are still unstable and need to be recomputed, and the order in which to do so. 
The chosen strategy is completely opaque to the data-flow framework to solve, but severely impacts the time it takes to arrive at a stable solution.
Commonly, the nodes which need to be updated are tracked within a \emph{worklist} in order not to update currently stable nodes.
The order in which the worklist processes unstable nodes also affects performance \parencite{dfa}.
The caching of analysis results between iterations \parencite{dmd} is a nice side-effect of the explicit graph abstraction.

Additionally, associated with each node is a \emph{transfer functions} which has the single responsibility of recomputing analysis information in terms of the current state of its dependencies.

Driven by the looming complexity in analysis order, we decided to break down our analysis in a similar manner.
In an effort to specify transfer functions separate from the iteration strategy, we arrived at the (slightly simplified) interface in \cref{fig:worklist}.

\begin{figure}[h]
  \centering
  \begin{haskellcode}
    runFramework 
      :: Ord node
      -> DataFlowFramework node lattice 
      -> Set node 
      -> Map node lattice
    runFramework = ...

    data DataFlowFramework node lattice = DFF 
          (node -> TransferFunction node lattice lattice)
          (node -> ChangeDetector node lattice)

    type ChangeDetector node lattice
      = lattice -> lattice -> Bool

    data TransferFunction node lattice a
      = TFM (State (WorklistState node lattice) a) -- not exported!
      deriving (Functor, Applicative, Monad)

    dependOn
      :: Ord node 
      => node 
      -> TransferFunction node lattice (Maybe lattice)
    dependOn
  \end{haskellcode}
  \caption{The essence of the \texttt{utils/Worklist.hs} module.}
  \label{fig:worklist}
\end{figure}

A \hsinl{DataFlowFramework} assigns to each abstract \hsinl{node} a \hsinl{TransferFunction} and a \hsinl{ChangeDetector} that compares the old value with the updated value to detect when the node has become stable.

Although we purposefully named the type variable denoting the analysis domain \hsinl{lattice}, we don't actually require it to model any kind of algebraic structure.
This is something we come back to in \cref{sec:mono}\tod{come back to it}.

The first two type parameters of \hsinl{TransferFunction} specify in what kind of data-flow framework the function operates, while the last parameter corresponds to the return value of that \hsinl{TransferFunction}.
Talking about what a transfer function `returns' might not make much sense just yet, but the derived \hsinl{Monad} instance hints at how this will turn out.
As always, a \hsinl{Monad} instance is useful to weave effects into otherwise pure computations. 
We can see that \hsinl{TransferFunction} is just an opaque wrapper around a stateful computation, but have no way of conjuring such a side-effect apart from cheating our way in with \hsinl{pure}.

The single means for introducing a `side-effect' is through \hsinl{dependOn}, which announces an edge in the data-flow graph by referencing the value of another node.
Depending on whether the iteration strategy can provide a value (at least cycles in the graph have to be broken at some point), it may or may not return a value, in which case the \hsinl{TransferFunction} is obliged to continue with an optimistic approximation (\eg $\bot$).

Note that just by executing the \hsinl{TransferFunction} in another \hsinl{WorklistState}, we can iterate a transfer function in a completely isolated manner.
The iteration strategy decides if the value of a node is immediately to be recomputed in a call to \hsinl{dependOn}, or if a value from a prior iteration is to be returned, or none at all.

The \hsinl{Ord} instance on \hsinl{node}, apart from being necessary for use as key in a \hsinl{Map}, serves as a priority on the nodes in the worklist.
By choosing a total order that mirrors how an analysis would proceed along the syntax tree, we are promised fast convergence by GHC's Occurence Analyser which arranged bindings in a suitable order.

In practice, we modeled each single point of a usage transformer as a separate node. 
This resulted in a specialisation of \hsinl{node} to \hsinl{(FrameworkNode, Use)}\footnote{Of course, the total order on \hsinl{Use} is not actually compatible with the join-semilattice we usually refer to and doesn't have any semantic meaning.}, where the total order on \hsinl{FrameworkNode} mirrors the syntax tree, and \hsinl{lattice} to \hsinl{(UsageType, CoreExpr)}, where the returned \hsinl{CoreExpr} is annotated with the findings of the analysis.
As it turned out, we lost important structure in forgetting about monotonicity (\cf \cref{sec:mono}).

\todo{Where we set FrameworkNodes and what ChangeDetectors, some example graph probably}
