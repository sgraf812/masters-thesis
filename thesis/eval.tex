\chapter{Evaluation}\label{sec:eval}

With implementation considerations sorted out in \cref{sec:impl}, we turn to assessing the analysis by means of the performance of the generated code in \cref{sec:bench}.
We also discuss compiler performance in \cref{sec:compperf} and see ways to trade artifact performance for analysis performance.

\section{Benchmarks}\label{sec:bench}

Enough with the mathy chit chat, give me numbers!

In this section we will look at how the analysis influences benchmark results in GHC's nofib \parencite{nofib} benchmark suite.

The (at the time of writing most recent) GHC 8.2.1 release -- revision \texttt{0cee252}, tagged as \texttt{ghc-8.2.1-release} -- serves as a baseline. We compare to three variants of our fork\footnote{Available at https://github.com/sgraf812/ghc under the branches \texttt{cocall-full}, \texttt{cocall-approximate-calls} and \texttt{cocall-complete-graphs}.}:

\begin{enumerate}
  \item[\varfull] 
    The most precise form of our analysis. 
    Replaces Call Arity entirely and is run two additional times, immediately after the Demand Analyser. 
    The usage information the Demand Analyser produces is only accessed to check for regressions.
  \item[\varcalls]
    This variant discards any product use at call sites. 
    E.g., $C^1(U(1*U,A))$ will be truncated to $C^1(U)$ in a call like \hsinl{let f x = ... in fst (f 42)}. 
    This is so that less nodes in the data-flow framework are accessed, to save space and time.
    Results should still show an improvement compared to the baseline.
  \item[\varedges]
    In addition to the changes of \varcalls, remove the co-call graph from the usage type. 
    This is equivalent to a co-call graph that only tracks self-edges and conservatively assumes the existence of all other co-call edges.
    The intention is to measure if the space and time impact of co-call graphs matters in practice.
    Note that regressions in some cases are expected, as the baseline employs Call Arity with accurate modelling of edges in co-call graphs at least between `interesting' variables \parencite[Section~3.4.1]{callarity}.
\end{enumerate}

\begin{table}
  \centering
  \begin{tabular}{lrrrrrr}
    \toprule
            & \multicolumn{3}{c}{Bytes allocated} & \multicolumn{3}{c}{Instructions executed} \\
              \cmidrule(lr){2-4} \cmidrule(lr){5-7}
    Program & \multicolumn{1}{c}{\varfull} & \multicolumn{1}{c}{\varcalls} & \multicolumn{1}{c}{\varedges} & \multicolumn{1}{c}{\varfull} & \multicolumn{1}{c}{\varcalls} & \multicolumn{1}{c}{\varedges} \\
    \midrule
    \input{benchmarks/usage-anal-nofib-table.tex}
    \bottomrule
  \end{tabular}
  \caption{
    Benchmark results of running nofib where the number improved by more than 0.3\% or regressed by more than 0.0\%.
    The GHC 8.2.1 release on which all work is based was used as a baseline.
    Variants \varfull to \varedges are increasingly approximate, but theoretically speaking, only \varedges may possibly yield worse results than the baseline's combination of Call Arity and Demand Analysis.
  }
  \label{tbl:nofib}
\end{table}

\Cref{tbl:nofib} shows the interesting results of running nofib.
We will break them down by discussing allocations and instructions executed separately.

\subsection{Allocations}\label{sec:alloc}

The total impact on allocations of our rather complex analysis is rather meager, with a reduction that doesn't even exceed the 0.1\% margin in the geometric mean over all benchmarks.
But recalling the goals of this thesis, namely unifying both Call Arity \parencite{callarity} and the work of \textcite{card} into a single analysis, this is good news and asserts our claims in \cref{sec:generalise}!

\begin{table}
  \centering
  \begin{tabular}{lrrr}
    \toprule
            & \multicolumn{3}{c}{Bytes allocated} \\
              \cmidrule(lr){2-4}
    Program & \multicolumn{1}{c}{\varfull} & \multicolumn{1}{c}{\varcalls} & \multicolumn{1}{c}{\varedges} \\
    \midrule
    \input{benchmarks/usage-anal-nofib-table-alloc.tex}
    \bottomrule
  \end{tabular}
  \caption{
    Interesting allocation results from the same run as \cref{tbl:nofib}.
    The only runs that were excluded were those with improvements of less than 0.1\%.
  }
  \label{tbl:alloc}
\end{table}

Allocation results are summarised in \cref{tbl:alloc}.
There is one tiny regression to allocations due to heuristics:
A partial application in \progname{infer} is regarded cheap to duplicate by GHC.
Therefore, our analysis spots a new opportunity for $\eta$-expansion and the binding gets inlined subsequently.
Although the resulting program allocates more than before, the number of executed instructions (\cf \cref{tbl:nofib}) went slightly down over all.

The most significant improvement happened to the allocations of \progname{fluid}, with a 1.5\% reduction due to arity expansion of some parsing function.

It is more instructive to look at \progname{fft2}. 
The 0.9\% improvement in allocations comes from an expression within \texttt{dfth} similar to the following program:
\begin{haskellcode}
  module Main (main) where

  p x = ...

  c x y = 
    if p x 
    then \z -> y 
    else \z -> c y z
  
  f x = 
    if p x 
    then c x (f (x-1))
    else \z -> x

  main = 
    print (f 1 2)
\end{haskellcode}

Our analysis finds out that \hsinl{f} can be $\eta$-expanded to arity 2.
Neither Call Arity nor the Demand Analyser recognises this.

Call Arity doesn't have any mechanism similar to usage signatures, so it assumes a conservative call arity of 1 for \hsinl{f} because of the call \hsinl{c x (f (x-1))}.

The Demand Analyser, on the other hand, has available only the usage signature for manifest arity 2, which is too conservative and unleashes a usage of $1*U$ on \hsinl{f (x-1)} instead of $1*C^1(U)$.

Our analysis infers from the call to \hsinl{c} with incoming arity 3 a call use of $C^1(U)$ for \hsinl{f (x-1)}, exposing \hsinl{f} to a total usage of $C^\omega(C^1(U))$, which we can leverage by expanding the arity to 2.

In expanding the analogue of \hsinl{f} within \texttt{dfth}, the analysis enables additional inlining and other transformations.
\smallskip

Apart from the very modest extreme cases, it is worth pointing out that allocation remained the same throughout all variants!
This is extremely important for practicality of the analysis, as we will see in \cref{sec:compperf}.

Although the implementation of \hsinl{foldl} contains a \hsinl{oneShot} annotation as a hint for the compiler, the same example motivating Call Arity in \textcite[Section~3.5.1]{callarity} is still $\eta$-expanded based on the results of variant \varedges:
\begin{haskellcode}
  let go x =
       let r = 
            if x == 2016
            then id 
            else go (x + 1)
       in if f x 
          then \a -> r (a + x)
          else r
  in go 42 0
\end{haskellcode}

GHC produces this code when compiling the expression \hsinl{sum (filter f [42..2016])}, where \hsinl{sum = foldl (+) 0} and \hsinl{foldl} is implemented in terms of \hsinl{foldr}.
For reasons outlined by \textcite{callarity}, it is crucial for performance to $\eta$-expand \hsinl{go}, so that no closure is allocated for \hsinl{r}.

Analysing the above expression under use $U$ with our variant \varedges will proceed in the following order:

\begin{enumerate}
  \item Since \hsinl{go} is exposed to an optimistic usage of $1*C^1(C^1(U))$, it will analyse \hsinl{go}'s bound expression under use $C^1(C^1(U))$.
  \item 
    This translates into usage of $1*C^1(U)$ on \hsinl{r}, in both branches of the \hsinl{if} expression. 
    Notably, the \hsinl{then} branch will first peel off one layer of the one-shot call use, then analyse \hsinl{r (a + x)} under use $U$ to find out the usage of $1*C^1(U)$ on \hsinl{r}.
    This alone would be enough evidence to $\eta$-expand \hsinl{r}.
  \item Analysing the bound expression of \hsinl{r} in use $C^1(U)$ will expose \hsinl{go} to a usage of $1*C^1(C^1(U))$. 
  \item After some fixpointing, \hsinl{r} gets its single-entry $1*C^1(U)$ annotation and \hsinl{go} will have an annotation of $\omega*C^\omega(C^1(U))$.
\end{enumerate}

According to our definition of $\expand_\sUsage$ in \cref{sec:generalise}, \hsinl{go}'s bound expression can be $\eta$-expanded, enabling vital further optimisations.

\subsection{Instructions Executed}\label{sec:instr}

Regarding instructions executed (measured with \texttt{cachegrind}), there are more significant improvements, although that might not reflect in performance on an actual machine.
In the geometric mean we achieve a total improvement of 0.6\%, which still is rather modest, but as said earlier not the primary goal of this thesis.

\begin{table}
  \centering
  \begin{tabular}{lrrr}
    \toprule
            & \multicolumn{3}{c}{Instructions executed} \\
              \cmidrule(lr){2-4}
    Program & \multicolumn{1}{c}{\varfull} & \multicolumn{1}{c}{\varcalls} & \multicolumn{1}{c}{\varedges} \\
    \midrule
    \input{benchmarks/usage-anal-nofib-table-instrs.tex}
    \bottomrule
  \end{tabular}
  \caption{
    Interesting programs with respect to instructions executed, from the same run as \cref{tbl:nofib}.
    Excluded were those runs with improvements of less than 3\% and regressions of less than 1\%.
  }
  \label{tbl:instr}
\end{table}

Let's focus instead on the most significant outliers in \cref{tbl:instr}.
With 11.4\% more instructions executed, \progname{fannkuch-redux} from the benchmarks game regresses significantly.

Looking at the Core output, it is not obvious to see where that regression comes from.
There were no subsequent transformations kicked off by the added annotations, and the annotations seem to be correct, judging from sprinkling a few \hsinl{Debug.Trace.trace} calls.
However when considering the decrease in allocations, it seems that GHC's heuristic cost model could be responsible for this, if not in some imported \texttt{base} module.

Conversely, the cases where performance improves are more frequent.
The largest boost receives \progname{gen\_regexps}, with 28.1\% less executed instructions.
Yet again, it is not appearent where those improvements come from. 
All annotations in the Core output seem proper and beyond that no further transformation happened.
This suggests that either the backend (the implementation of which the author is blissfully unaware) or some optimisation within referenced \texttt{base} modules is responsible.

The same arguments apply to the 15.4\% improvement in \progname{puzzle}:
A number of new annotations that didn't enable any further Core-to-Core transformation.
The set of functions referenced from \texttt{base} is quite minimal, so it is unlikely that optimisations hide there.

\subsection{Compiler Performance}\label{sec:compperf}

The graph data structure necessary for modelling co-call graphs is quite complicated, optimised for sparse and dense graphs to offer even remotely manageable compiler performance.

Yet for some input files, time and -- worse -- space complexity goes through the roof:
The space needed to compile some innocent file like \texttt{GHC/Types.hs} from \texttt{ghc-prim} goes up from a few hundred megabytes to multiple gigabytes.
Clearly, this is unacceptable for a mature and widely used compiler like GHC.

Just before finalising this thesis, we also realised that variant \varfull is not even able to compile an optimised version of the stage 2 compiler!
Memory usage while compiling the \hsinl{LlvmCodeGen} module exceeded the 32 GB of our build machine, probably because of the involvement of \hsinl{DynFlags}, a huge tuple.
That is why we use GHC's \texttt{bench} build flavour for our benchmarks, which still generate completely optimised artifacts with an unoptimised stage 2 compiler.

Heap profiling revealed that the explosion is indeed related to co-call graphs, peaks in memory needed to store \hsinl{IntMap}s, in particular.
Practically all of the overhead is attributed to \hsinl{UsageAnal.Types.bothUsageType}, which computes the union of three graphs on each invocation. 
It is being used quite often, for example as part of the expensive graph substitution procedure.

Even in a version of \varedges where the co-call graphs still were present in a dumb fashion (\eg just tracking self-edges), \hsinl{UnVarGraph.completeBipartiteGraph} seemed to be responsible for the major blowup, even though the quadratic space complexity should have been eliminated.

\begin{table}
  \centering
  \begin{tabular}{lrrrrrr}
    \toprule
            & \multicolumn{3}{c}{Bytes allocated} & \multicolumn{3}{c}{Instructions executed} \\
              \cmidrule(lr){2-4} \cmidrule(lr){5-7}
    Program & \multicolumn{1}{c}{\varfull} & \multicolumn{1}{c}{\varcalls} & \multicolumn{1}{c}{\varedges} & \multicolumn{1}{c}{\varfull} & \multicolumn{1}{c}{\varcalls} & \multicolumn{1}{c}{\varedges} \\
    \midrule
    \input{benchmarks/usage-anal-nofib-comp-table.tex}
    \input{benchmarks/usage-anal-ghc-comp-table.tex}
    \bottomrule
  \end{tabular}
  \caption{
    Performance figures recorded while compiling stage 2 GHC (unoptimised, but with optimised libraries) and nofib (completely optimised) with the three variants, relative to GHC 8.2.1.
    Resource usage of \varfull and \varcalls grows beyond limits in specific cases, accounting for a major blowup overall.
    The most approximate variant, \varedges, shows an acceptable increase, knowing that there are opportunities left for making it more efficient.
  }
  \label{tbl:compperf}
\end{table}

With that out of the way, we may risk a look at \cref{tbl:compperf}, which shows the performance figures recorded while compiling itself and nofib.

Compiling and optimising nofib seems to have caused a greater regression (between 10 to 20 percent) than compiling the stage 2 compiler (only 1.5 to 2.6 percent).
That is likely the case because we used the \texttt{bench} build flavour, which optimises the dependent libraries but not the stage 2 compiler.
This means our optimisations are not run on GHC itself, which is entirely due to the above mentioned space explosion in variant \varfull; the other variants compiled just fine.
Again, the performance figures regarding nofib are unaffected by this.

Other than that, every variant consumes more resources than the baseline. 
That is not surprising: 
We replaced a single run of Call Arity by three runs of our analysis, of which a single run can hardly be cheaper.
However, the Demand Analyser still performs Cardinality Analysis on every of its two runs, which could be left out.

There is still plenty of opportunity left to reach acceptable performance.
Our variant \varedges, for example, doesn't exhibit the same problematic space complexity as the other variants, because it gets rid of co-call graphs altogether.
It turns out that call arity aware \letdnsc-style analysis (as in \varedges) is precise enough to optimise all important cases!

The whole graph-based data-flow iteration story doesn't yet run as performant as it could.
Currently, a \hsinl{Map} is used to map nodes to transfer functions.
If all syntax nodes would provide sensible \hsinl{FrameworkNode}s, modeling the graph as a fixed-size array of monotone maps is possible.
Also, the whole \hsinl{FrameworkBuilder} incantation would be obsolete if \hsinl{CoreExpr} carried appropriate \hsinl{FrameworkNode}s.

All in all, it is too early to judge the replacement for Cardinality Analysis and Call Arity by compiler performance.

